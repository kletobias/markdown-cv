{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0763b4f6-98de-42ac-b4e7-56147c6058b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ipyparallel.client.client.Client at 0x106fbc5b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipyparallel as ipp\n",
    "\n",
    "cluster = ipp.Cluster.from_file(\"/Users/tobias/.ipython/profile_default/security/cluster-1676630660-cbwm.json\")\n",
    "rc = cluster.connect_client_sync()\n",
    "rc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e84e14d7-49f3-4b03-9174-9baf52e786b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ipyparallel\n",
    "# !pip install markdown2\n",
    "# !pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !ipcluster start -n 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "648713eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process: 39844.\n"
     ]
    }
   ],
   "source": [
    "# from ipyparallel import Client\n",
    "# rc = Client()\n",
    "%px\n",
    "rc.ids\n",
    "import os\n",
    "print(f\"Process: {os.getpid():d}.\")\n",
    "import itertools\n",
    "import re\n",
    "from markdown2 import Markdown\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aeaf7e2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51b8bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir='/Users/tobias/all_code/projects/portfolio-website-2022/_projects/'\n",
    "files = os.listdir(dir)\n",
    "host = 'https://deep-learning-mastery.com/projects/'\n",
    "# for f in files:\n",
    "#     print(f)\n",
    "#     print(f'is .md: {f[-3:] == \".md\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37a6e0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cl(files,host):\n",
    "    md_files = []\n",
    "    link = {}\n",
    "    for f in files:\n",
    "        if f[-3:] == '.md':\n",
    "            md_files.append(f[:-3])\n",
    "            link[f[:-3]] = host + f'{f[:-3]}' + '/'\n",
    "        else:\n",
    "            continue\n",
    "    linkf = []\n",
    "    for k,v in link.items():\n",
    "        linkf.append(f'[{k}]({v})')\n",
    "\n",
    "\n",
    "    return linkf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "726653ea",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[data_prep_4](https://deep-learning-mastery.com/projects/data_prep_4/)', '[regression60min_datacamp-concrete-challenge](https://deep-learning-mastery.com/projects/regression60min_datacamp-concrete-challenge/)', '[data_prep_1](https://deep-learning-mastery.com/projects/data_prep_1/)', '[tabular_kaggle-7](https://deep-learning-mastery.com/projects/tabular_kaggle-7/)', '[tabular_kaggle-3](https://deep-learning-mastery.com/projects/tabular_kaggle-3/)', '[tabular_kaggle-2](https://deep-learning-mastery.com/projects/tabular_kaggle-2/)', '[tabular_kaggle-6](https://deep-learning-mastery.com/projects/tabular_kaggle-6/)', '[theory_batch_gradient_descent](https://deep-learning-mastery.com/projects/theory_batch_gradient_descent/)', '[advanced-geospatial-feature-creation](https://deep-learning-mastery.com/projects/advanced-geospatial-feature-creation/)', '[tabular_kaggle-1](https://deep-learning-mastery.com/projects/tabular_kaggle-1/)', '[2nd_tm](https://deep-learning-mastery.com/projects/2nd_tm/)', '[mysql-redshift-1](https://deep-learning-mastery.com/projects/mysql-redshift-1/)', '[tabular_kaggle-5](https://deep-learning-mastery.com/projects/tabular_kaggle-5/)', '[tabular_kaggle-4](https://deep-learning-mastery.com/projects/tabular_kaggle-4/)', '[1st_tm](https://deep-learning-mastery.com/projects/1st_tm/)', '[data_prep_2](https://deep-learning-mastery.com/projects/data_prep_2/)', '[data_prep_3](https://deep-learning-mastery.com/projects/data_prep_3/)']\n"
     ]
    }
   ],
   "source": [
    "mdf = cl(files,host)\n",
    "print(mdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b47bdc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_description(dir,files):\n",
    "    patt = re.compile('^title:\\s[\\'\\\"](.+)[\\'\\\"]$',re.IGNORECASE)\n",
    "    patd = re.compile('^description:\\s[\\'\\\"](.+)[\\'\\\"]$',re.IGNORECASE)\n",
    "    patc = re.compile('^category:\\s\\[[\\'\\\"](.+)[\\'\\\"]\\]$',re.IGNORECASE)\n",
    "    patta = re.compile('^tags:\\s(\\[.+\\])$',re.IGNORECASE)\n",
    "    patsub = re.compile('<br>')\n",
    "    pat_html = re.compile(r'<[^>]+>|\\\\n',re.MULTILINE)\n",
    "    md_files = []\n",
    "    cv_text_all = []\n",
    "    td = {}\n",
    "    link = {}\n",
    "    linkf = []\n",
    "    for f in files:\n",
    "        if f[-3:] == '.md':\n",
    "            td[f[:-3]] = {}\n",
    "            md_files.append(f[:-3])\n",
    "            td[f[:-3]]['url'] = f'[Full Article]({host}{f[:-3]}/)'\n",
    "            with open(os.path.join(dir,f),'r') as a:\n",
    "                article = a.readlines()\n",
    "                doc = nlp(pat_html.sub('',str(article)))\n",
    "                words = [token.text for token in doc if token.is_stop != True and token.is_punct != True]\n",
    "                td[f[:-3]]['word_count'] = len(words)\n",
    "            with open(os.path.join(dir,f),'r') as ff:\n",
    "                for line in itertools.islice(ff,2,8):\n",
    "                    ttl = patt.search(line)\n",
    "                    ddl = patd.search(line)\n",
    "                    ccl = patc.search(line)\n",
    "                    tal = patta.search(line)\n",
    "                    if ttl != None:\n",
    "                        ttls = patsub.sub(' ',ttl[1])\n",
    "                        td[f[:-3]][\"title\"] = ttls\n",
    "                    elif ddl != None:\n",
    "                        ddls = patsub.sub(' ',ddl[1])\n",
    "                        td[f[:-3]][\"description\"] = ddls\n",
    "                    elif ccl != None:\n",
    "                        ccls = patsub.sub(' ',ccl[1])\n",
    "                        td[f[:-3]][\"category\"] = ccls\n",
    "                    elif tal != None:\n",
    "                        tals = patsub.sub(' ',tal[1])\n",
    "                        td[f[:-3]][\"tags\"] = tals\n",
    "                    else:\n",
    "                        continue\n",
    "    for k,v in link.items():\n",
    "        linkf.append(f'[{k}]({v})')\n",
    "    for k in sorted(td.keys()):\n",
    "        print(td[k].items())\n",
    "        cv_text = f'\\\n",
    "<br>\\\n",
    "<H4>{td[k][\"title\"]}</H4>\\\n",
    "**Description:** {td[k][\"description\"]}\\\n",
    "<br>\\\n",
    "                **Tags:**  {td[k][\"tags\"]}<br>\\\n",
    "                **Category:** *{td[k][\"category\"]}* | **Word Count:**\\\n",
    "                {td[k][\"word_count\"]} | **{td[k][\"url\"]}**<br>\\\n",
    "                <br><br>'\n",
    "        md = Markdown()\n",
    "        print(f'CONVERTED: {md.convert(cv_text)}')\n",
    "        cv_text_all.append(md.convert(cv_text))\n",
    "\n",
    "    with open('cv_articles.md','w+') as f:\n",
    "        for item in cv_text_all:\n",
    "            f.write(item)\n",
    "    return cv_text_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f5412ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('url', '[Full Article](https://deep-learning-mastery.com/projects/1st_tm/)'), ('word_count', 2703), ('title', 'Automation Using A Test Harness For Deep Learning: Part 1'), ('description', 'How to create and use a custom test harness, that automates many steps of the deep learning testing process. It lowers GPU idle time, lets one build more models, test more parameter combinations in less time. The fastai library for deep learning is used throughout this article.'), ('tags', \"['binary-classification', 'deep-learning', 'fastai', 'hyperparameter-optimization', 'learning-rate', 'loss-function', 'stochastic-gradient-descent']\"), ('category', 'deep-learning')])\n",
      "CONVERTED: <p><br><H4>Automation Using A Test Harness For Deep Learning: Part 1</H4><strong>Description:</strong> How to create and use a custom test harness, that automates many steps of the deep learning testing process. It lowers GPU idle time, lets one build more models, test more parameter combinations in less time. The fastai library for deep learning is used throughout this article.<br>                <strong>Tags:</strong>  ['binary-classification', 'deep-learning', 'fastai', 'hyperparameter-optimization', 'learning-rate', 'loss-function', 'stochastic-gradient-descent']<br>                <strong>Category:</strong> <em>deep-learning</em> | <strong>Word Count:</strong>                2703 | <strong><a href=\"https://deep-learning-mastery.com/projects/1st_tm/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "dict_items([('url', '[Full Article](https://deep-learning-mastery.com/projects/2nd_tm/)'), ('word_count', 3060), ('title', 'Automation Using A Test Harness For Deep Learning: Part 2'), ('description', 'This is Part 2 in the series, where we explore how the fastai deep learning library can be used to conduct structured empirical experiments on a novel and small dataset. The dataset consists of 850 images and an almost uniform distribution for the target labels. There are two labels in total, \"male\" and \"female\", that are assigned the gender of the model depicted in any of the images in the dataset.'), ('tags', \"['binary-classification', 'deep-learning', 'fastai', 'hyperparameter-optimization','image-data']\"), ('category', 'deep-learning')])\n",
      "CONVERTED: <p><br><H4>Automation Using A Test Harness For Deep Learning: Part 2</H4><strong>Description:</strong> This is Part 2 in the series, where we explore how the fastai deep learning library can be used to conduct structured empirical experiments on a novel and small dataset. The dataset consists of 850 images and an almost uniform distribution for the target labels. There are two labels in total, \"male\" and \"female\", that are assigned the gender of the model depicted in any of the images in the dataset.<br>                <strong>Tags:</strong>  ['binary-classification', 'deep-learning', 'fastai', 'hyperparameter-optimization','image-data']<br>                <strong>Category:</strong> <em>deep-learning</em> | <strong>Word Count:</strong>                3060 | <strong><a href=\"https://deep-learning-mastery.com/projects/2nd_tm/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "dict_items([('url', '[Full Article](https://deep-learning-mastery.com/projects/advanced-geospatial-feature-creation/)'), ('word_count', 3777), ('title', 'Advanced Geospatial Feature Creation'), ('description', 'Extensive cleaning and transformation of tabular data, in order to create geospatial features. Once processed, the results are clean GPS values as \"Point\" objects in decimal degrees format and names of all subway and suburban train stations within Hamburg, Germany.'), ('tags', \"['data-cleaning', 'data-transformation', 'geospatial-feature-creation', 'regular-expression', 'shapely', 'tabular-data']\"), ('category', 'data-preprocessing')])\n",
      "CONVERTED: <p><br><H4>Advanced Geospatial Feature Creation</H4><strong>Description:</strong> Extensive cleaning and transformation of tabular data, in order to create geospatial features. Once processed, the results are clean GPS values as \"Point\" objects in decimal degrees format and names of all subway and suburban train stations within Hamburg, Germany.<br>                <strong>Tags:</strong>  ['data-cleaning', 'data-transformation', 'geospatial-feature-creation', 'regular-expression', 'shapely', 'tabular-data']<br>                <strong>Category:</strong> <em>data-preprocessing</em> | <strong>Word Count:</strong>                3777 | <strong><a href=\"https://deep-learning-mastery.com/projects/advanced-geospatial-feature-creation/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "dict_items([('url', '[Full Article](https://deep-learning-mastery.com/projects/data_prep_1/)'), ('word_count', 3443), ('title', 'Cleaning a webscraped 47 Column Pandas DataFrame Part 1'), ('description', 'Data Preparation Series: Exploring Tabular Data With pandas: An Overview Of Available Tools In The pandas Library.'), ('tags', \"['data-exploration', 'first-steps', 'introduction', 'pandas', 'tabular-data']\"), ('category', 'data-preprocessing')])\n",
      "CONVERTED: <p><br><H4>Cleaning a webscraped 47 Column Pandas DataFrame Part 1</H4><strong>Description:</strong> Data Preparation Series: Exploring Tabular Data With pandas: An Overview Of Available Tools In The pandas Library.<br>                <strong>Tags:</strong>  ['data-exploration', 'first-steps', 'introduction', 'pandas', 'tabular-data']<br>                <strong>Category:</strong> <em>data-preprocessing</em> | <strong>Word Count:</strong>                3443 | <strong><a href=\"https://deep-learning-mastery.com/projects/data_prep_1/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "dict_items([('url', '[Full Article](https://deep-learning-mastery.com/projects/data_prep_2/)'), ('word_count', 3198), ('title', 'Cleaning a webscraped 47 Column Pandas DataFrame Part 2'), ('description', 'More efficient string data cleaning by using the pyjanitor module and method chaining.'), ('tags', \"['data-cleaning', 'pandas', 'regular-expressions', 'string-manipulation', 'tabular-data']\"), ('category', 'data-preprocessing')])\n",
      "CONVERTED: <p><br><H4>Cleaning a webscraped 47 Column Pandas DataFrame Part 2</H4><strong>Description:</strong> More efficient string data cleaning by using the pyjanitor module and method chaining.<br>                <strong>Tags:</strong>  ['data-cleaning', 'pandas', 'regular-expressions', 'string-manipulation', 'tabular-data']<br>                <strong>Category:</strong> <em>data-preprocessing</em> | <strong>Word Count:</strong>                3198 | <strong><a href=\"https://deep-learning-mastery.com/projects/data_prep_2/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "dict_items([('url', '[Full Article](https://deep-learning-mastery.com/projects/data_prep_3/)'), ('word_count', 2338), ('title', 'Cleaning a webscraped 47 Column Pandas DataFrame Part 3'), ('description', 'Extensive cleaning and validation and creation of a valid GPS column from the records, by joining the longitude and latitude columns together using geometry object Point.'), ('tags', \"['data-validation', 'dtype-timedelta64','geospatial-feature-engineering', 'pandas', 'tabular-data']\"), ('category', 'data-preprocessing')])\n",
      "CONVERTED: <p><br><H4>Cleaning a webscraped 47 Column Pandas DataFrame Part 3</H4><strong>Description:</strong> Extensive cleaning and validation and creation of a valid GPS column from the records, by joining the longitude and latitude columns together using geometry object Point.<br>                <strong>Tags:</strong>  ['data-validation', 'dtype-timedelta64','geospatial-feature-engineering', 'pandas', 'tabular-data']<br>                <strong>Category:</strong> <em>data-preprocessing</em> | <strong>Word Count:</strong>                2338 | <strong><a href=\"https://deep-learning-mastery.com/projects/data_prep_3/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "dict_items([('url', '[Full Article](https://deep-learning-mastery.com/projects/data_prep_4/)'), ('word_count', 4192), ('title', 'Cleaning a webscraped 47 Column Pandas DataFrame Part 4'), ('description', 'Extensive data cleaning and validation using regular expressions. Showcase of how batch processing several columns of tabular data using pandas, pyjanitor and the re library can look like.'), ('tags', \"['batch-processing', 'data-validation', 'pandas', 'regular-expressions', 'tabular-data']\"), ('category', 'data-preprocessing')])\n",
      "CONVERTED: <p><br><H4>Cleaning a webscraped 47 Column Pandas DataFrame Part 4</H4><strong>Description:</strong> Extensive data cleaning and validation using regular expressions. Showcase of how batch processing several columns of tabular data using pandas, pyjanitor and the re library can look like.<br>                <strong>Tags:</strong>  ['batch-processing', 'data-validation', 'pandas', 'regular-expressions', 'tabular-data']<br>                <strong>Category:</strong> <em>data-preprocessing</em> | <strong>Word Count:</strong>                4192 | <strong><a href=\"https://deep-learning-mastery.com/projects/data_prep_4/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "dict_items([('url', '[Full Article](https://deep-learning-mastery.com/projects/mysql-redshift-1/)'), ('word_count', 1982), ('title', 'MySQL Queries Using An AWS Redshift MySQL Database'), ('description', 'This article shows how one can use Python to import CSV files using Pandas into a MySQL database hosted on AWS using Redshift and how to formulate basic MySQL queries to get the data of interest.'), ('tags', \"['mysql', 'AWS', 'pandas', 'tabular-data', 'query']\"), ('category', 'tabular-data')])\n",
      "CONVERTED: <p><br><H4>MySQL Queries Using An AWS Redshift MySQL Database</H4><strong>Description:</strong> This article shows how one can use Python to import CSV files using Pandas into a MySQL database hosted on AWS using Redshift and how to formulate basic MySQL queries to get the data of interest.<br>                <strong>Tags:</strong>  ['mysql', 'AWS', 'pandas', 'tabular-data', 'query']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                1982 | <strong><a href=\"https://deep-learning-mastery.com/projects/mysql-redshift-1/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "dict_items([('url', '[Full Article](https://deep-learning-mastery.com/projects/regression60min_datacamp-concrete-challenge/)'), ('word_count', 1162), ('title', 'Datacamp Concrete Regression Challenge'), ('description', 'This is the notebook I created to solve the datacamp concrete challenge within an hour. There are explanations for most of the code in this article and we look deeper into the workings of the Lasso regression model.'), ('tags', \"['cross-validation', 'lasso-regression', 'math', 'multivariate-regression', 'regression-analysis']\"), ('category', 'tabular-data')])\n",
      "CONVERTED: <p><br><H4>Datacamp Concrete Regression Challenge</H4><strong>Description:</strong> This is the notebook I created to solve the datacamp concrete challenge within an hour. There are explanations for most of the code in this article and we look deeper into the workings of the Lasso regression model.<br>                <strong>Tags:</strong>  ['cross-validation', 'lasso-regression', 'math', 'multivariate-regression', 'regression-analysis']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                1162 | <strong><a href=\"https://deep-learning-mastery.com/projects/regression60min_datacamp-concrete-challenge/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "dict_items([('url', '[Full Article](https://deep-learning-mastery.com/projects/tabular_kaggle-1/)'), ('word_count', 5305), ('title', 'Deep Dive Tabular Data Pt. 1'), ('description', 'Preprocessing Data: Visualizing missing values on an 80 feature dataset. Strategies for filling missing values and using categorical embeddings.'), ('tags', \"['categorical-embeddings', 'data-preprocessing', 'fastai', 'fill-strategies', 'tabular-data']\"), ('category', 'tabular-data')])\n",
      "CONVERTED: <p><br><H4>Deep Dive Tabular Data Pt. 1</H4><strong>Description:</strong> Preprocessing Data: Visualizing missing values on an 80 feature dataset. Strategies for filling missing values and using categorical embeddings.<br>                <strong>Tags:</strong>  ['categorical-embeddings', 'data-preprocessing', 'fastai', 'fill-strategies', 'tabular-data']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                5305 | <strong><a href=\"https://deep-learning-mastery.com/projects/tabular_kaggle-1/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "dict_items([('url', '[Full Article](https://deep-learning-mastery.com/projects/tabular_kaggle-2/)'), ('word_count', 1839), ('title', 'Deep Dive Tabular Data Pt. 2'), ('description', 'Feature selection using model DecisionTreeRegressor from sklearn and the feature\\\\_importances\\\\_ method which is tested for deviations in its score.'), ('tags', \"['decision-tree-regressor', 'feature-importance', 'feature-selection', 'sklearn', 'tabular-data']\"), ('category', 'tabular-data')])\n",
      "CONVERTED: <p><br><H4>Deep Dive Tabular Data Pt. 2</H4><strong>Description:</strong> Feature selection using model DecisionTreeRegressor from sklearn and the feature_importances_ method which is tested for deviations in its score.<br>                <strong>Tags:</strong>  ['decision-tree-regressor', 'feature-importance', 'feature-selection', 'sklearn', 'tabular-data']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                1839 | <strong><a href=\"https://deep-learning-mastery.com/projects/tabular_kaggle-2/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "dict_items([('url', '[Full Article](https://deep-learning-mastery.com/projects/tabular_kaggle-3/)'), ('word_count', 1351), ('title', 'Deep Dive Tabular Data Pt. 3'), ('description', 'RandomForestRegressor using feature\\\\_importances\\\\_ and out-of-bag error to asses model performance.'), ('tags', \"['feature-importance', 'feature-selection', 'out-of-bag-error', 'random-forest', 'tabular-data']\"), ('category', 'tabular-data')])\n",
      "CONVERTED: <p><br><H4>Deep Dive Tabular Data Pt. 3</H4><strong>Description:</strong> RandomForestRegressor using feature_importances_ and out-of-bag error to asses model performance.<br>                <strong>Tags:</strong>  ['feature-importance', 'feature-selection', 'out-of-bag-error', 'random-forest', 'tabular-data']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                1351 | <strong><a href=\"https://deep-learning-mastery.com/projects/tabular_kaggle-3/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "dict_items([('url', '[Full Article](https://deep-learning-mastery.com/projects/tabular_kaggle-4/)'), ('word_count', 1607), ('title', 'Deep Dive Tabular Data Pt. 4'), ('description', 'Interpretation Using Advanced Statistical Visualizations. Dendrogram, Spearman rank correlation, partial dependence plot, impact of independent variables for sample on predictions.'), ('tags', \"['dendrogram', 'partial-dependence', 'spearman-rank-correlation', 'tabular-data', 'treeinterpreter']\"), ('category', 'tabular-data')])\n",
      "CONVERTED: <p><br><H4>Deep Dive Tabular Data Pt. 4</H4><strong>Description:</strong> Interpretation Using Advanced Statistical Visualizations. Dendrogram, Spearman rank correlation, partial dependence plot, impact of independent variables for sample on predictions.<br>                <strong>Tags:</strong>  ['dendrogram', 'partial-dependence', 'spearman-rank-correlation', 'tabular-data', 'treeinterpreter']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                1607 | <strong><a href=\"https://deep-learning-mastery.com/projects/tabular_kaggle-4/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "dict_items([('url', '[Full Article](https://deep-learning-mastery.com/projects/tabular_kaggle-5/)'), ('word_count', 1030), ('title', 'Deep Dive Tabular Data Pt. 5'), ('description', 'Out-of-domain problem: What it is, why it is important, how to spot it and how to deal with it.'), ('tags', \"['feature-importance', 'model-accuracy', 'out-of-domain-problem', 'random-forest', 'tabular-data']\"), ('category', 'tabular-data')])\n",
      "CONVERTED: <p><br><H4>Deep Dive Tabular Data Pt. 5</H4><strong>Description:</strong> Out-of-domain problem: What it is, why it is important, how to spot it and how to deal with it.<br>                <strong>Tags:</strong>  ['feature-importance', 'model-accuracy', 'out-of-domain-problem', 'random-forest', 'tabular-data']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                1030 | <strong><a href=\"https://deep-learning-mastery.com/projects/tabular_kaggle-5/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "dict_items([('url', '[Full Article](https://deep-learning-mastery.com/projects/tabular_kaggle-6/)'), ('word_count', 2647), ('title', 'Deep Dive Tabular Data Pt. 6'), ('description', 'Kaggle Submission 1: Training RandomForestRegressor, fastai deep learning model using hyperparameter optimization techniques. Preprocessing of Kaggle test data.'), ('tags', \"['data-preprocessing', 'fastai', 'hyperparameter-optimization', 'random-forest', 'tabular-data']\"), ('category', 'tabular-data')])\n",
      "CONVERTED: <p><br><H4>Deep Dive Tabular Data Pt. 6</H4><strong>Description:</strong> Kaggle Submission 1: Training RandomForestRegressor, fastai deep learning model using hyperparameter optimization techniques. Preprocessing of Kaggle test data.<br>                <strong>Tags:</strong>  ['data-preprocessing', 'fastai', 'hyperparameter-optimization', 'random-forest', 'tabular-data']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                2647 | <strong><a href=\"https://deep-learning-mastery.com/projects/tabular_kaggle-6/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "dict_items([('url', '[Full Article](https://deep-learning-mastery.com/projects/tabular_kaggle-7/)'), ('word_count', 3459), ('title', 'Deep Dive Tabular Data Pt. 7'), ('description', 'Kaggle Submission 2: tabular\\\\_learner deep learning estimator optimized using manual hyperparameter optimization. XGBRegressor using RandomizedSearchCV and sampling from continuous parameter distributions.'), ('tags', \"['hyperparameter-optimization', 'random-search', 'tabular-data', 'tabular_learner', 'xgboost-regressor']\"), ('category', 'tabular-data')])\n",
      "CONVERTED: <p><br><H4>Deep Dive Tabular Data Pt. 7</H4><strong>Description:</strong> Kaggle Submission 2: tabular_learner deep learning estimator optimized using manual hyperparameter optimization. XGBRegressor using RandomizedSearchCV and sampling from continuous parameter distributions.<br>                <strong>Tags:</strong>  ['hyperparameter-optimization', 'random-search', 'tabular-data', 'tabular_learner', 'xgboost-regressor']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                3459 | <strong><a href=\"https://deep-learning-mastery.com/projects/tabular_kaggle-7/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "dict_items([('url', '[Full Article](https://deep-learning-mastery.com/projects/theory_batch_gradient_descent/)'), ('word_count', 1756), ('title', 'The Math Behind \"Stepping The Weights\"'), ('description', 'In this article we highlight a key concept in the Stochastic Gradient Descent and explore the basics, that this optimization algorithm is derived of.'), ('tags', \"['deep-learning', 'math', 'ordinary-least-squares', 'partial-derivate', 'stochastic-gradient-descent']\"), ('category', 'deep-learning')])\n",
      "CONVERTED: <p><br><H4>The Math Behind \"Stepping The Weights\"</H4><strong>Description:</strong> In this article we highlight a key concept in the Stochastic Gradient Descent and explore the basics, that this optimization algorithm is derived of.<br>                <strong>Tags:</strong>  ['deep-learning', 'math', 'ordinary-least-squares', 'partial-derivate', 'stochastic-gradient-descent']<br>                <strong>Category:</strong> <em>deep-learning</em> | <strong>Word Count:</strong>                1756 | <strong><a href=\"https://deep-learning-mastery.com/projects/theory_batch_gradient_descent/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "\n",
      "<p><br><H4>Automation Using A Test Harness For Deep Learning: Part 1</H4><strong>Description:</strong> How to create and use a custom test harness, that automates many steps of the deep learning testing process. It lowers GPU idle time, lets one build more models, test more parameter combinations in less time. The fastai library for deep learning is used throughout this article.<br>                <strong>Tags:</strong>  ['binary-classification', 'deep-learning', 'fastai', 'hyperparameter-optimization', 'learning-rate', 'loss-function', 'stochastic-gradient-descent']<br>                <strong>Category:</strong> <em>deep-learning</em> | <strong>Word Count:</strong>                2703 | <strong><a href=\"https://deep-learning-mastery.com/projects/1st_tm/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "\n",
      "\n",
      "<p><br><H4>Automation Using A Test Harness For Deep Learning: Part 2</H4><strong>Description:</strong> This is Part 2 in the series, where we explore how the fastai deep learning library can be used to conduct structured empirical experiments on a novel and small dataset. The dataset consists of 850 images and an almost uniform distribution for the target labels. There are two labels in total, \"male\" and \"female\", that are assigned the gender of the model depicted in any of the images in the dataset.<br>                <strong>Tags:</strong>  ['binary-classification', 'deep-learning', 'fastai', 'hyperparameter-optimization','image-data']<br>                <strong>Category:</strong> <em>deep-learning</em> | <strong>Word Count:</strong>                3060 | <strong><a href=\"https://deep-learning-mastery.com/projects/2nd_tm/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "\n",
      "\n",
      "<p><br><H4>Advanced Geospatial Feature Creation</H4><strong>Description:</strong> Extensive cleaning and transformation of tabular data, in order to create geospatial features. Once processed, the results are clean GPS values as \"Point\" objects in decimal degrees format and names of all subway and suburban train stations within Hamburg, Germany.<br>                <strong>Tags:</strong>  ['data-cleaning', 'data-transformation', 'geospatial-feature-creation', 'regular-expression', 'shapely', 'tabular-data']<br>                <strong>Category:</strong> <em>data-preprocessing</em> | <strong>Word Count:</strong>                3777 | <strong><a href=\"https://deep-learning-mastery.com/projects/advanced-geospatial-feature-creation/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "\n",
      "\n",
      "<p><br><H4>Cleaning a webscraped 47 Column Pandas DataFrame Part 1</H4><strong>Description:</strong> Data Preparation Series: Exploring Tabular Data With pandas: An Overview Of Available Tools In The pandas Library.<br>                <strong>Tags:</strong>  ['data-exploration', 'first-steps', 'introduction', 'pandas', 'tabular-data']<br>                <strong>Category:</strong> <em>data-preprocessing</em> | <strong>Word Count:</strong>                3443 | <strong><a href=\"https://deep-learning-mastery.com/projects/data_prep_1/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "\n",
      "\n",
      "<p><br><H4>Cleaning a webscraped 47 Column Pandas DataFrame Part 2</H4><strong>Description:</strong> More efficient string data cleaning by using the pyjanitor module and method chaining.<br>                <strong>Tags:</strong>  ['data-cleaning', 'pandas', 'regular-expressions', 'string-manipulation', 'tabular-data']<br>                <strong>Category:</strong> <em>data-preprocessing</em> | <strong>Word Count:</strong>                3198 | <strong><a href=\"https://deep-learning-mastery.com/projects/data_prep_2/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "\n",
      "\n",
      "<p><br><H4>Cleaning a webscraped 47 Column Pandas DataFrame Part 3</H4><strong>Description:</strong> Extensive cleaning and validation and creation of a valid GPS column from the records, by joining the longitude and latitude columns together using geometry object Point.<br>                <strong>Tags:</strong>  ['data-validation', 'dtype-timedelta64','geospatial-feature-engineering', 'pandas', 'tabular-data']<br>                <strong>Category:</strong> <em>data-preprocessing</em> | <strong>Word Count:</strong>                2338 | <strong><a href=\"https://deep-learning-mastery.com/projects/data_prep_3/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "\n",
      "\n",
      "<p><br><H4>Cleaning a webscraped 47 Column Pandas DataFrame Part 4</H4><strong>Description:</strong> Extensive data cleaning and validation using regular expressions. Showcase of how batch processing several columns of tabular data using pandas, pyjanitor and the re library can look like.<br>                <strong>Tags:</strong>  ['batch-processing', 'data-validation', 'pandas', 'regular-expressions', 'tabular-data']<br>                <strong>Category:</strong> <em>data-preprocessing</em> | <strong>Word Count:</strong>                4192 | <strong><a href=\"https://deep-learning-mastery.com/projects/data_prep_4/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "\n",
      "\n",
      "<p><br><H4>MySQL Queries Using An AWS Redshift MySQL Database</H4><strong>Description:</strong> This article shows how one can use Python to import CSV files using Pandas into a MySQL database hosted on AWS using Redshift and how to formulate basic MySQL queries to get the data of interest.<br>                <strong>Tags:</strong>  ['mysql', 'AWS', 'pandas', 'tabular-data', 'query']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                1982 | <strong><a href=\"https://deep-learning-mastery.com/projects/mysql-redshift-1/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "\n",
      "\n",
      "<p><br><H4>Datacamp Concrete Regression Challenge</H4><strong>Description:</strong> This is the notebook I created to solve the datacamp concrete challenge within an hour. There are explanations for most of the code in this article and we look deeper into the workings of the Lasso regression model.<br>                <strong>Tags:</strong>  ['cross-validation', 'lasso-regression', 'math', 'multivariate-regression', 'regression-analysis']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                1162 | <strong><a href=\"https://deep-learning-mastery.com/projects/regression60min_datacamp-concrete-challenge/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "\n",
      "\n",
      "<p><br><H4>Deep Dive Tabular Data Pt. 1</H4><strong>Description:</strong> Preprocessing Data: Visualizing missing values on an 80 feature dataset. Strategies for filling missing values and using categorical embeddings.<br>                <strong>Tags:</strong>  ['categorical-embeddings', 'data-preprocessing', 'fastai', 'fill-strategies', 'tabular-data']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                5305 | <strong><a href=\"https://deep-learning-mastery.com/projects/tabular_kaggle-1/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "\n",
      "\n",
      "<p><br><H4>Deep Dive Tabular Data Pt. 2</H4><strong>Description:</strong> Feature selection using model DecisionTreeRegressor from sklearn and the feature_importances_ method which is tested for deviations in its score.<br>                <strong>Tags:</strong>  ['decision-tree-regressor', 'feature-importance', 'feature-selection', 'sklearn', 'tabular-data']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                1839 | <strong><a href=\"https://deep-learning-mastery.com/projects/tabular_kaggle-2/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "\n",
      "\n",
      "<p><br><H4>Deep Dive Tabular Data Pt. 3</H4><strong>Description:</strong> RandomForestRegressor using feature_importances_ and out-of-bag error to asses model performance.<br>                <strong>Tags:</strong>  ['feature-importance', 'feature-selection', 'out-of-bag-error', 'random-forest', 'tabular-data']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                1351 | <strong><a href=\"https://deep-learning-mastery.com/projects/tabular_kaggle-3/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "\n",
      "\n",
      "<p><br><H4>Deep Dive Tabular Data Pt. 4</H4><strong>Description:</strong> Interpretation Using Advanced Statistical Visualizations. Dendrogram, Spearman rank correlation, partial dependence plot, impact of independent variables for sample on predictions.<br>                <strong>Tags:</strong>  ['dendrogram', 'partial-dependence', 'spearman-rank-correlation', 'tabular-data', 'treeinterpreter']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                1607 | <strong><a href=\"https://deep-learning-mastery.com/projects/tabular_kaggle-4/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "\n",
      "\n",
      "<p><br><H4>Deep Dive Tabular Data Pt. 5</H4><strong>Description:</strong> Out-of-domain problem: What it is, why it is important, how to spot it and how to deal with it.<br>                <strong>Tags:</strong>  ['feature-importance', 'model-accuracy', 'out-of-domain-problem', 'random-forest', 'tabular-data']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                1030 | <strong><a href=\"https://deep-learning-mastery.com/projects/tabular_kaggle-5/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "\n",
      "\n",
      "<p><br><H4>Deep Dive Tabular Data Pt. 6</H4><strong>Description:</strong> Kaggle Submission 1: Training RandomForestRegressor, fastai deep learning model using hyperparameter optimization techniques. Preprocessing of Kaggle test data.<br>                <strong>Tags:</strong>  ['data-preprocessing', 'fastai', 'hyperparameter-optimization', 'random-forest', 'tabular-data']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                2647 | <strong><a href=\"https://deep-learning-mastery.com/projects/tabular_kaggle-6/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "\n",
      "\n",
      "<p><br><H4>Deep Dive Tabular Data Pt. 7</H4><strong>Description:</strong> Kaggle Submission 2: tabular_learner deep learning estimator optimized using manual hyperparameter optimization. XGBRegressor using RandomizedSearchCV and sampling from continuous parameter distributions.<br>                <strong>Tags:</strong>  ['hyperparameter-optimization', 'random-search', 'tabular-data', 'tabular_learner', 'xgboost-regressor']<br>                <strong>Category:</strong> <em>tabular-data</em> | <strong>Word Count:</strong>                3459 | <strong><a href=\"https://deep-learning-mastery.com/projects/tabular_kaggle-7/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "\n",
      "\n",
      "<p><br><H4>The Math Behind \"Stepping The Weights\"</H4><strong>Description:</strong> In this article we highlight a key concept in the Stochastic Gradient Descent and explore the basics, that this optimization algorithm is derived of.<br>                <strong>Tags:</strong>  ['deep-learning', 'math', 'ordinary-least-squares', 'partial-derivate', 'stochastic-gradient-descent']<br>                <strong>Category:</strong> <em>deep-learning</em> | <strong>Word Count:</strong>                1756 | <strong><a href=\"https://deep-learning-mastery.com/projects/theory_batch_gradient_descent/\">Full Article</a></strong><br>                <br><br></p>\n",
      "\n",
      "\n",
      "Controller stopped: {'exit_code': None, 'pid': 39233, 'identifier': 'ipcontroller-1676630660-cbwm-39844'}\n",
      "engine set stopped 1676630675: {'engines': {'0': {'exit_code': None, 'pid': 39246, 'identifier': '0'}, '1': {'exit_code': None, 'pid': 39247, 'identifier': '1'}, '2': {'exit_code': None, 'pid': 39248, 'identifier': '2'}, '3': {'exit_code': None, 'pid': 39249, 'identifier': '3'}, '4': {'exit_code': None, 'pid': 39253, 'identifier': '4'}, '5': {'exit_code': None, 'pid': 39258, 'identifier': '5'}, '6': {'exit_code': None, 'pid': 39262, 'identifier': '6'}, '7': {'exit_code': None, 'pid': 39268, 'identifier': '7'}, '8': {'exit_code': None, 'pid': 39274, 'identifier': '8'}, '9': {'exit_code': None, 'pid': 39280, 'identifier': '9'}}, 'exit_code': None}\n"
     ]
    }
   ],
   "source": [
    "td = get_title_description(dir,files)\n",
    "for item in td:\n",
    "    print()\n",
    "    print(item)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
